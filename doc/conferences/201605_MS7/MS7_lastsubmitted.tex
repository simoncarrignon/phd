\documentclass[a4paper]{article}


\title{Abstract MS7:\\ Computer modelling and simulation as heuristic tool to understand the past: the case of the EPNEt project.}
\author{Simon Carrignon, Alessandro Mosca, Bernardo Rondelli and Jos Remesal}
\date{October 2015}


\begin{document}
 



Incompleteness and uncertainty of the historical and archaeological record affect historical interpretation \cite{madella2014}. In this paper, we argue that \emph{formal modelling} and computer Simulation are valuable tools to overcome such limitations. We first show how evolutionary biologists have successfully embraced these tools and why their epistemological framework is close to what archaeologists and historians do. We then defend how those M\&S are good heuristic tools in science in general and we finished by introducing an interdisciplinary research setting, where the previous points can be exploited in a meaningful way to investigate historical questions.

The goal of evolutionary biologists is to understand the mechanisms at the origin of the living world as we can observe it. Assuming the theory of evolution, they characterise the succession of past events that constitute this story. Starting with Gould \cite{gould1989wonderfullife}, several biologists and philosophers have argued that the nature of this research activity is historical \cite{beatty1995evolutionary}: the actual biological world does not depend \emph{only} on biological rules, but on the uniqueness of the succession of events. To encompass the issues raised by such historicity, evolutionary biologists use, at least since the Modern Synthesis, formal models to figure out different possible successions of events and the likelihood of such possible historical paths, and they test it against the available data.

This suggest that (i) the problems encountered by evolutionary biologists are close to those archaeologists and historians have to face (ii) the way inferences are made about the history of living beings and the history of human societies fall into a similar epistemological framework and (iii) mathematical and computer models are good candidates to infer, in a statistically plausible and transparent way, missing data and complex hypotheses in both enterprise.

This use of computer simulations and modelling is not restricted to evolutionary biology. It is now widely spread in all branches of Science. People in artificial life \cite{paolo00simulationmodelsasopaquethoughtexperiments} argue that computer simulation are powerful heuristic tools that combine the exploratory power of thought experiments and the logical strength of mathematics. They allow to test quickly a lot of possible ``opaque though experiments'' that would be impossible to execute mentally. Moreover, in complex systems where the interactions of every subcomponents are multiple, the global dynamics are difficult to predict analytically, which make simulation and modelling one of the best suitable tool to explore and study those mechanisms.


However, building computational simulations that can provide valuable knowledge about the modelled object still remains a difficult task. Computer scientists have to be aware of every assumptions they could implicitly made and domain experts (as historians or biologists) have to formulate their hypotheses in an epistemological framework yet not clearly specified and far from the one they are used. The communication is thus primordial: knowledge here does not lie in the mathematical models neither in the historical data but emerges from the well articulation of both side \cite{winsberg09taletwomethods}.

Here we provide examples from the EPNet project, where the emphasis is on supporting historians with computational infrastructure for understanding the political and economical implications behind food production and distribution along the Roman Empire. By taking into consideration the design and development of such a computational infrastructure, the EPNet epistemological framework is aiming to address three main problems: (i) structuring and making accessible large collections of data through the Web, (ii) providing a formally defined, unambiguous, framework for analysing the data and exporting them in a way that can be further manipulated by computer simulation algorithms, and complex network analysis, and (iii) making each collection of data integrable with other complementary data sources.


\bibliographystyle{ieeetr}
\bibliography{/home/scarrign/projects/PhD/doc/biblio/bib/phd.bib}  




\end{document}





