<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0055)http://cogprints.org/1617/1/harnad00.mind.humphrey.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   
   <meta name="GENERATOR" content="Mozilla/4.08 [en] (X11; I; IRIX 5.3 IP22) [Netscape]">
<style type="text/css"></style></head>
<body>
Harnad, S. (2000) Correlation vs. Causality: How/Why the Mind/Body Problem
Is Hard. <i>Journal of Consciousness Studies
</i>7(4): 54-61. <b>Copyright
2000 Imprint Academic.</b>
<br>[Invited Commentary on Humphrey, N. "<a href="http://cogprints.soton.ac.uk/archives/phil/papers/200002/200002001/doc.html/mindbodytxt.htm">
How to Solve the Mind-Body Problem </a>". (As an afterthought, I rather
wished I had called my commentary "How to Solve the Mind-Body Problem Is
Hard".)]
<hr>
<center>
<p><b>CORRELATION VS. CAUSALITY: HOW/WHY THE MIND/BODY PROBLEM IS HARD</b>
</p><p>Stevan Harnad
<br>Cognitive Sciences Center
<br>Department of Electronics and Computer Science
<br>Southampton University
<br>Highfield, Southampton
<br>SO17 1BJ United Kingdom
<br><a href="mailto:harnad@soton.ac.uk">harnad@soton.ac.uk</a>
<br><a href="http://cogsci.soton.ac.uk/~harnad">http://cogsci.soton.ac.uk/~harnad</a></p></center>

<blockquote><i>"brain-imaging studies... demonstrate in ever more detail
how specific kinds of mental activity (as reported by a mindful subject)
are precisely correlated with specific patterns of brain activity (as recorded
by external instruments)</i>.(Humphrey 2000)<b>"</b></blockquote>
Mind/Brain (M/B) correlations: We've known about them (dimly) for decades,
probably centuries. And that's still all we've got with brain imaging;
and that's all we'll have even when we get the correspondence fine-tuned
right down to the last mental jnd ("just-noticeable difference") and its
corresponding molecule.
<p>But the Mind/Body Problem (M/BP) is about <u>causation</u> not correlation.
And its solution (if there is one) will require a mechanism in which the
mental component somehow manages to play a causal role of its own, rather
than just supervening superflously on other, nonmental components that
look, for all the world, as if they can do the full causal job perfectly
well without it (thank you very much). Correlations confirm that M does
indeed "supervene" on B, but causality is needed to show how/why M is not
supererogatory; and that's the hard part.
</p><p>Nick Humphrey's heroic attempt is informative -- and some of it might
even be correct, functionally speaking -- but alas it too fails to furnish
the missing causal link for the mental component, which continues to dangle
in his account, nonfunctionally. Hence the only problems Humphrey solves
are the "easy" ones, not the M/BP.
</p><blockquote><i>"suppose, by analogy, that... "atmospheric-imaging" experiments
[demonstrate] that whenever there is a visible shaft of lightning in the
air there is a corresponding electrical discharge. We might soon be confident
that the lightning and the electrical discharge are aspects of one and
the same thing"</i></blockquote>
Such analogies are (famously) inapplicable to the M/BP (Nagel 1974, 1986):
There is no problem about seeing two sets of empirical observations as
"aspects" of the same thing, given a causal model that unifies them. But
there is no such causal model in the case of the M/BP. For, unlike all
other empirical observations, such as lightning/electricity (or water/H2O,
heat/molecular-motion, life/biogenetic-function, matter/energy, etc.),
in the special case of M/B, the correlated phenomena are not of the same
KIND. And that's precisely what makes this particular set of "correlations"
different, and problematic. So the forecast that M/B will simply turn out
to be yet another set of correlations like the rest is unpromising.
<p>Empirically detectable shafts of lightning and empirically detectable
electrical discharges are the same kind of thing (empirical data, detectable
by instruments). So are empirically detectable brain activities and empirically
detectable behaviour and circumstances. So when I say or act out that something
hurts (especially when something is indeed damaging my tissues), and the
accompanying brain-image is neural activity in my nociceptor system, we
do have a correlation between things of the same kind, exactly as in the
case of the lightning. And out of that correlation we can contruct a causal
theory of nociceptive function (tissue injury, avoidance, learning, recall,
etc.).
</p><p>But when the correlate in question is my <u>feeling</u> of pain, we're
in another ballpark: There's now an explanatory gap that neither the nociceptive
theory (which is only a functional theory of tissue-damage-related <u>doing</u>)
nor any amount of reconfirmation of the tightness of the correlation can
close.
</p><p>So maybe what Humphrey means to highlight in the lightning case is not
the correlation between the two sets of empirical data, but between the
empirical data and an underlying causal factor that explains the data.
That's fine too, but then the analogy with M/B correlations in imaging
is irrelevant, and we are talking about a causal explanation: And if so,
what IS that underlying causal factor in the case of the M/BP? All I see
is unexplicated correlations. (Note that the neural functions and the behavioral
functions and their interrelations do get explicated, but their <u>mental</u>
correlations do not.)
</p><p>Nor is it a matter of "deducing one from the other a priori" (even physics
doesn't do that, only mathematics does). The "laws" of physics are not
necessary but contingent; so are their boundary conditions. So this is
a red herring. The real problem is about causal explanation in the special
case of M/B. Consciousness seems fated to be a causal dangler no matter
how tight the coupling and how minute the predictability. It's like perfect
weather-forecasting without the underlying meteorological theory.
</p><p>(Remember: Neural and behavioral functions are not at issue; mental
ones are. The correlations of a purely behavioral neuroscience would not
be problematic in any way; it's the causal status of the <u>mental </u>component
that is at the root of the M/BP. The causality in M/B theory is invariably
"third-party" causality: The underlying neural mechanism causes both the
brain/body's functional neural/behavioral states <u>and</u> the fact that
they happen to be mental states. The trick is to show -- <u>functionally</u>,
if that's the route one elects to take -- how/why the mentality is <u>not</u>
functionally superfluous; just reaffirming that its causally hard-wired
somehow to its functional substrate is not an answer.)
</p><blockquote><i>"with lightning [t]he physico-chemical causes that underlie
the identity [were] discovered through further experimental research and
new theorizing. Now the question is whether the same strategy will work
for mind and brain."</i></blockquote>
It is indeed, and the question arises because of the obvious <u>dis</u>analogies:
public ("3rd person") data, as in <u>all </u>the other analogies, vs. private
("1st person") data. There's a start. And then there's the disanalogy about
the (independent) causal role of the private-stuff ("qualia" = feelings):
it had better not try to exert any, on pain of telekinetic dualism. Which
leaves the usual question of why it's dangling there, then, epiphenomenally.
A "functionalist" would have expected a better answer, a <u>functional</u>
one. But it's whenever we try to face squarely the question of what causal
role feelings could possibly have that we draw a blank (unless we cheat
by "identifying" feelings with something <u>else</u> -- such as a neural
correlate, thereby begging the question!).
<p>Humphrey wishes to distance his own position from two nonstarters, those
of Chalmers (1996) and McGinn (1989):
</p><blockquote><i>"Chalmers [1996]... argues [that] consciousness just happens
to be a fundamental, non-derivative, property of matter."</i></blockquote>
One must agree with Humphrey that such dicta are unedifying. Here is a
one-line summary of Chalmers's message: "The M/BP is <u>hard</u>!" So what?
How does that help? It's a good pull-up, for those who have simplistic
quick-fixes, but other than that it is tautological: It wouldn't be the
longstanding problem it is if it weren't "hard." The question is: Is it
soluble at all?
<blockquote><i>"McGinn [1989] believes that... certain kinds of understanding...
must for ever lie beyond our intellectual reach [e.g., the M/BP]."</i></blockquote>
Little substance in that position either, in my opinion. I too happen to
think the M/BP's insoluble, but not because of any limitation of the human
mind. Indeed, I don't know what is even meant by saying that there may
indeed exist a "solution" to the M/BP, but not one that the mind can ever
know! There is nothing here that is analogous in any way to the (epistemic?)
constraints underlying Goedel unprovability, quantum indeterminacy, statistical-mechanical
indeterminacy, unproven mathematical conjectures, halting problems, the
many-body problem, the limits of measurement, the limits of memory, the
limits of technology, the limits of computation, NP-completeness, the limits
of time, the limits of "language" (no idea what the last might even mean)
etc. Those are all red herrings and false analogies.
<p>Nor is it clear why in his own approach Humphrey wants to obscure the
M/BP with formalism ("dimensions," "equations," "identities")? The problem
is clear, hard, and staring us informally in the face: I have feelings.
Undoubtedly the feelings are in some way caused by and identical to ("supervene
on") brain process/structures, but it is not at all clear <u>how</u>, and
even less clear <u>why</u>. That's the M/BP. No "equation" to write down;
no "terms." And the "incommensurability" is the name of the game (or of
the problem)!
</p><p>Inputs and outputs can be connected, functionally, computationally.
But feelings are another story (a hard one!). If we "characterize" feelings
computationally or functionally, we have simply begged the question, and
changed the subject -- to a discussion of the relation between brain function
and computational (or other) function.
</p><blockquote><i>"Most of the states of interest to psychologists... remembering,
perceiving, wanting, talking, thinking, and so on are... amenable to...
functional analysis."</i></blockquote>
In every respect except the <u>relevant</u> one, which is that they are
<u>qualitative</u>,
feeling states. They will be amenable to <u>informational</u> analysis,
and to <u>behavioural</u> and <u>neural</u> analysis, but their feelingness
will remain a dangler -- and that's the point! That's what makes the M/BP
the problem it is. The functional stuff would all go through fine -- behaviourally,
computationally -- <u>if</u> we were all just feelingless Zombies. But
we're not. And that's the problem (Harnad 1995, 2000).
<blockquote><i>"recalling that today is Tuesday = activity of neurons in
the calendula nucleus.... But [these] are notoriously the "easy" cases...
No one it seems has the least idea how to characterize the phenomenal&nbsp;&nbsp;
experience of redness in functional terms"</i></blockquote>
This is too quick. There is <u>nothing special</u> about "detecting red,"
compared to "recalling that X" or "inferring that Y." These can <u>all
</u>be
treated functionally (i.e., as transpiring in a Zombie), in precisely the
same way. There are I/O conditions under which certain psychophysical capabilities
in the "chromoceptive task" domain are adaptive for our species, hence
our brains have evolved the functional mechanisms for processing objects
with reflective surfaces, etc. But why/how does doing and being able to
do that kind of thing <u>feel-like</u> something? Back to square one, and
it's exactly the <u>same</u> square for both the "cognitive/intentional"
cases Humphrey thinks are easier (recalling X), and the more patently phenomenal/qualitative
ones that wear their hardness more on their sleeves (detecting red).
<p>In reality, <u>every</u> mental capacity has both an easy and a hard
<u>aspect</u>:
the functional aspect is easy, the feeling aspect is hard. But it's the
feeling aspect that makes it mental! So there's only one M/BP, and that's
the hard one. The rest is just mindless Zombie functionalism (a branch
of reverse bioengineering that is not particularly "easier" empirically
than any other area of science).
</p><p>Now we arrive at what will be the core insight, which Humphrey attributes
to Reid:
</p><blockquote>"<i>sensory awareness is an activity. We do not have pains
we get to be pained. This is an extraordinarily sophisticated insight [of
Reid's]"</i></blockquote>
In my opinion Reid provides no insight here. The M/BP has always abutted
onto various forms of scepticism -- scepticism about the external world,
scepticism about other minds. The problem of "hallucination" (an apparent
external object of experience, when in reality there is no external object)
has sometimes been folded into the M/BP. From this follows this eagerness
to distinguish external object-based experiences like feeling tree-barks
from completely internal ones, like feeling moods (I leave out the awkward
intermediate case of feeling headaches).
<p>No illumination follows from adopting these distinctions. YES some of
the sceptical problems (not induction, but definitely solipsism and other-minds)
are lemmas of the M/BP. But the M/BP is primary. Solve that and those bits
of idealism will be trivial by comparison (and will probably vanish).
</p><p>But don't try to subordinate the (unsolved) theorem to the derivative
lemma! Never mind the distinction between trees and moods: For M/B purposes
they are completely of a muchness, and <u>moods</u> are the more representative
case, rather than the external-world-contingent special case of trees.
</p><p>To put it another way: it is the relation between feelings and brain
states that we are charged with explicating, not the relation between feelings
and their <u>objects</u> whether internal or external (i.e., "feeling that
I am seeing a blue balloon" vs. "feeling [affectively] blue"). <u>All </u>such
cases are equally symptomatic of and infected with the M/BP; to try instead
to resolve some of the differences that distinguish them from one another
them is just to change the subject and beg the question (and analogous
to focusing on differences in notation instead of confronting their common
content, except that here it is qualitative content differences that are
distactctng us from the problem --which is that there esists any content
at all!)).
</p><blockquote><i>"my own view... is that the right expression is not so much
"being pained" as "paining"... sensing is not a passive state at all, but...
active engagement with the stimulus occurring at the body surface."</i></blockquote>
As GB Shaw said: "Madame, we have already established your profession,
we are merely haggling about the price." Call it what one likes -- call
seeing a tree "tree-seeing," call feeling a pain "paining" -- there is
no illumination in sight in this corridor, just gerunds! (The profession,
here, is unfortunately question-begging functionalism.)
<p>And let us quickly lay to rest what might have looked as if it too were
a contender for a phenomenological category of its own, along with seeing
trees vs. feeling pains: performing a volitional act (e.g., lifting my
finger). No help there either. When I lift my finger, it feels-like "me-deliberately-finger-lifting."
Just another feeling to account for, along with all the others. The fact
that it feels-like I'm <u>doing </u>it, rather than like it's being d<u>one-unto</u>
me amounts to just one more (irrelevant) difference in feeling content.
It does <u>not</u> give us any leg up on the M/BP.
</p><p>Efference is a red herring here. It feels-like I'm being the agent,
and maybe that's correlated with efferent brain activity: yet another correlation.
(Humphrey's proposed "Sentition" is even worse. We don't need new terms!
We need conceptual insights -- if there are any to be had.) I too happen
to have a longstanding interest in the motor component of perception (Harnad
1982). But there are no inroads to the M/BP from any of that -- just perhaps
true and interesting functional facts about the relation between afference
and efference, reafference, reflexive vs. nonreflexive behavior, motor
theories of perception, etc.
</p><p>One also cannot agree with Humphrey that "it is 'like something' to
have sensations, but not like anything much to engage in most other bodily
activities!" Kinesthesia is qualitative; so is the difference between what
it feels-like to raise your leg when it has been tapped on the patella
by a doctor vs. when you will the motion deliberately. Those are all differences
in "Feeling Space" -- just like everything else (including mental imagery
and mental reasoning, indeed all of the "language of thought"). But this
casts no light on the M/BP.
</p><p>Humphrey's "self-resonance" sounds like just another one in the long
litany of "self-X" terms meant to illuminate consciousness (self-awareness,
self-reference, self-representation, etc.) -- while in reality merely renaming
it.
</p><p>(I might add that the "self" need not figure in it at all: The concept
of the "self" -- though, like everything, it has its qualitative contents
-- surely came late in the evolutionary day. The amphyoxus already has
a full-blown M/B problem, even though it does not know it, if it aches
when you pinch it, and that's the only experience that ever goes on in
there -- no Cartesian reflection on its being <u>my</u> ache, and <u>me</u>
as distinct from <u>it</u> as the <u>patient</u> of that <u>sensation</u>,
etc. But just that ache is feeling enough; no "self-resonance" needed...)
</p><p>Similarly, the fact that an "animal has a defining edge to it, a structural
boundary" sounds like an excellent functional/Darwinian reason for evolving
mechanisms that make the inner-outer distinction, and act upon it. But
why should any of that <u>feel</u> like anything? Why must one <u>not</u>
be a <u>zombie-amoeba</u> in order to get the full functional benefits
Humphrey describes in his evolutionary scenario? (As usual, there is some
conflation of the mental and the internal here.)
</p><p>Organisms "must evolve the ability to sort out the good from the bad
and to respond... with an ow": Humphrey is here caught in the act of cognostic
contraband, smuggling in a <u>feeling</u> reaction (otherwise what does
the "ow" mean?), where all that was needed functionally was a <u>doing</u>
one. Question duly begged.
</p><p>Similarly with "When red light falls on it... it wriggles redly.": Why
should that <u>feel </u>like anything? And if it doesn't, then it's just
wriggling wrigglingly, under red conditions. Once one has cheated, and
allowed <u>any</u> qualitative light to enter into and "quicken" what should
merely have been functional/Darwinian survival/reproduction machines --
Zombie ones, as plants are [I hope, being a vegeterian who tries not to
eat anything that has or has ever had qualia!] -- generating the mental
stuff one has set out planning to explain, the game is over and the M/B
question is begged! Why/how do they wriggle <u>feelingly</u> rather than
merely <u>doingly</u>?
</p><blockquote><i>"as yet, these sensory responses are nothing other than
responses... no reason to suppose that the animal is in any way mentally
aware of what is happening."</i></blockquote>
A bit of equivocation here: Can one be "aware" in any other way than "mentally"?
And just what is it that is "happening" if it is not the object of any
awareness? If no one is feeling anything, then the events in question might
just as well be transpiring on the other side of the moon as within an
entity's head, for there is no one home there either. So these are Zombies
wriggling wrigglingly under red conditions, not wriggling redly. No light
means no light, not just paler light.
<blockquote><i>"as this animal's life becomes more complex, the time comes
when it will indeed be advantageous for it to have some kind of inner knowledge
of what is affecting it, which it can begin to use as a basis for more
sophisticated planning and decision making. So it needs the capacity to
form mental representations of the sensory stimulation at the surface of
its body and how it feels about it."</i></blockquote>
How it <u>feels </u>about it? But this was supposed to be the functional
explication of what it <u>is </u>to feel! In reality, it sounds as if,
for functional reasons, the entity needs certain internal structures and
processes. Fine, but why should it <u>feel-like</u> something to have those,
or to have them activated (Harnad 1996)? As before, why/how are these not
just Zombies with internal structures and processes that do whatever it
is that needs functional doing? "Internal" certainly does not mean "mental,"
as every thermostat knows (or rather doesn't).
<blockquote><i>"By monitoring its own responses, it forms a representation
of 'what is happening to me'."</i></blockquote>
We've already settled earlier on the functional utility of an internal
distinction between external and internal. But why should <u>that</u> feel-like
anything either? (And if it doesn't, the "me" has no subject.)
<blockquote><i>"wouldn't it be better off if, besides being aware of feeling
the pressure wave as such"</i></blockquote>
This is again equivocal between functionally-responsive-to the pressure
wave and <u>feeling</u> it.
<blockquote><i>"able to interpret this stimulus as signaling an approaching
predator?"</i></blockquote>
"Interpret" is again equivocal. Functionally, it just means process the
information and compute the result. Why should that involve or engender
<u>feelings?</u>
<p>It should be clear by now that Humphrey has gotten out of this exactly
what he has put into it. His language and caveats are equivocal about precisely
where, how, or why he has smuggled in the light of consciousness (or the
warmth of feelings), but clearly at some point he has, and we are meant
to go along with this.
</p><p>Alas, I cannot, because Humphrey has given me no reason -- functional
or logical -- for doing so. He has simply arbitrarily turned on the mental
lights at some juncture, and somehow attributed that to the Darwinian story
he was telling; yet the story (except as a Just-So story. i.e., as mere
hermeneutics) does not explain or justify it at all.
</p><blockquote>"<i>When the question is "what is happening to me?", the answer
that is wanted is qualitative, present-tense, transient, and subjective.
When the question is "what is happening out there?", the answer that is
wanted is quantitative, analytical, permanent, and objective."</i></blockquote>
No account whatsoever is given of why this should be the case. Why do internal
data have to have the mental lights on, whereas external data do not? (I
don't even think it's true, in that external-object-event-processing is
probably just as closely correlated with consciousness as internal.)
<p>It is conceivable that two systems evolved along the lines Humphrey
describes; it is even conceivable that there are some correlations between
his functional account and consciousness (although even functionally some
parts seem problematic).
</p><p>But the part Humphrey owes us, if this is really meant to have any bearing
on the M/B problem, is an explanation of when/how/why <u>feelings</u> kicked
in (whatever their correlation with these two hypothetical systems might
be).
</p><blockquote><i>"proto-experience of sensation arises from its monitoring
its own command signals for these sensory responses."</i></blockquote>
"Proto" is a weasel word here: Are we talking about feelings, or about
something else? I do not know, nor can I conceive, of anything intermediate
or "proto" in between feeling and nonfeeling. And it would be sensation
when the mental lights went on, not "experience of sensation," which is
redundant. With the mental lights off, sensation is just "event" or "physical
effect" or "response." Optical transducers respond to light; they don't
have sensations. There's no one in there to be the subject of sensations,
to feel; and sensations are experiences, feelings, full-blown. (Self-monitoring
is an old favorite. But why <u>feelingly</u> self-monitoring, rather than
<u>zombily</u>?)
<blockquote><i>"though the animal may no longer want to respond directly
to the stimulation at its body surface as such, it still wants to be able
to keep up to date mentally with what's occurring"</i></blockquote>
Delays, planning, monitoring, policing: All good stuff, and we ourselves
all certainly do it mentally; but that's neither here nor there. How/why
is it mental rather than just internal but mindless Zombie adaptation here,
in this putative explication? (These are the <u>hard</u> questions.) In
his functional account, Humphrey has described some very useful internal
computations, but he has left out entirely how/why they have anything feelingful
about them. Till he can can do that, the M/BP has been untouched by any
of this.
<p>Humphrey's closed-circuits and internal loops are also popular candidates
for "self-X" structures/processes (self-modifying and self-organizing are
others), but all of these are too easy! One cannot just baptize them as
"mental" and declare that a problem has been solved. It is the simplest
thing (and indeed valid, because of the M/B correlation) to give a mentalistic
interpretation to certain brain processes. But to <u>interpret</u> them
as mental (even <u>truly</u> so) is not to <u>explain</u> them (causally)
as mental! Such are the limits of mentalistic hermeneutics Harnad 1990a,
b).
</p><blockquote><i>"Now once this happens [natural] selection is no longer
involved in determining the form of these responses and... the quality
of the representations based on them."</i></blockquote>
This is a reasonable rationale for replacing further evolutionary adaptation
by all-purpose learning and intelligence, based on evolved internal cognitive
mechanisms, but not a hint about why/how any of this is conscious (Harnad
2000).
<p>I never quite figured out what Humphrey's "thickness factor" was (perhaps
there was something intentionally Geertzian about it [Geertz 1973]) --
apart from the fact that it appears to be making some sort of a continuum
out of something that is all or none: Either feeling is going on or it
is not; if it is, then we are again just haggling about the price; but
Humphrey owes us an explanation of how/why the feeling-switch was turned
on at all in the first place.
</p><blockquote><i>"when the process becomes internalized and the circuit so
much shortened, the conditions are there for a significant degree of recursive
interaction to come into play... the command signals for sensory responses
begin to loop back upon themselves, becoming in the process partly self-creating
and self-sustaining... they have... become signals about themselves."</i></blockquote>
I can design and implement recursive, self-sustaining loops fitting Humphrey's
description easily. Do they quicken with the light of consciousness too?
If not, then why/how do the ones Humphrey says do, do? Animating the "self-X"
words does not explain, it covers up! And its not getting signals that
are "about" themselves that is the problem -- for this very sentence is
now about itself too; the problem is getting someone in there for those
signals to be about something <u>to</u>: a conscious subject. That problem
is hard, and alas, Nick Humphrey, like everyone else so far, has failed
to solve it.
<p>
</p><hr>
<p>Reply to Humphrey's Reply:
</p><blockquote>&nbsp;
<br><i><b>HUMPHREY</b>: Sometimes, I confess, that when faced with the
only-up-to-a-point functionalism of van Gulick or de Quincey or most of
my colleagues, I am almost relieved to come across the unrepentant in-your-face
anti-functionalism of Stevan Harnad. At least with Harnad you know where
you are.</i></blockquote>

<p><br>With this problem, you either solve it or you don't; no half-way
houses on the hard road to consciousness.
</p><p>And it won't do to dismiss me as an antifunctionalist either! I <i>am</i>
a functionalist -- for the <i>doable</i> part, which is the "easy" problem
(though no easier than the rest of science), namely, the reverse-engineering
of the brain's capacity for doing. It is the hopeless, hapless tilting
at the hard problem of feeling, with functionalism as a lance, that I am
afraid I must continue to decline as having absolutely no explanatory power
for me...
<br>&nbsp;
</p><blockquote><i><b>HUMPHREY</b>: The sarcasm is transparent: `</i>I can
design and implement recursive self-sustaining loops fitting Humphrey's
description easily. Do they quicken with the light of consciousness?<i>'
No of course not, because: `</i>If we "characterize" feelings computationally
or functionally, we have simply begged the question, and changed the subject'.
<i>Functionalism
for Harnad is by definition </i>'zombie functionalism'<i>.</i></blockquote>

<p><br>Not quite by definition; by default. I'm ready to add feelings the
minute there is a FUNCTIONAL call for them; but as long as the feelings
are merely decorative or hermeneutic or "supervenient," as long as there
is no reason at all that the functions cannot take care of all the doings
feelinglessly, the hard problem remains intact.
<br>&nbsp;
</p><blockquote><i><b>HUMPHREY: </b>But, if I am glad to have Harnad state
the enemy's case so boisterously and scornfully as he does in his commentary,
it's only because he thereby reveals the ultimate vacuity of his position.
It goes nowhere. It makes no predictions. It generates no tests. Indeed,
for Harnad it would actually be an argument against the legitimacy of any
theory of consciousness that someone should even imagine that his theory
could be tested by implementing the consciousness-producing architecture
in a machine. Because if he were to interpret anything the machine actually
does with its new architecture (anything at all) as evidence that the implementation
has been successful, that would only show that his theory begged the question.</i></blockquote>
Alas the hard and the easy problem are hopelessly conflated here. Predictions
and tests can show that function has been successfully implemented (that's
the "easy" part), but they cannot show how/why any of it is feelingful
(that's the hard part). Merely interpreting it as feelingful won't do the
trick.
<br>&nbsp;
<blockquote><i><b>HUMPHREY</b>: It's as though Harnad has managed to turn
Tertullian's grand claim 'I believe because it is impossible' into its
corollary 'I do not believe because it is possible'.</i></blockquote>
I do not believe until/unless feelings can be shown to be functional. I
have heard no attempt to do this (other than telekinetic dualism, a nonstarter,
on physical and probabilistic grounds).
<p>(Note again that it is not in the existence of feelings, nor even in
the presence of feelings in the successful functional implementation, that
I disbelieve, but in the success of Humphrey's&nbsp; functional explication
of feelings, i.e., his putative solution to the MB/P.)
<br>&nbsp;
</p><blockquote><i><b>HUMPHREY: </b>Of course this was also the ultimate argument
used against Darwin. It may be true, the churchmen said, that it would
have been possible for the living world to have been designed by natural
selection. But don't be fooled. God has arranged things to appear as-if
designed by natural selection, just so as to test your faith in the fact
that they have not really been so designed.</i></blockquote>
This gets it upside-down: Feelings are real, not as-if. Functionality is
real too. It is the hermeneutics of functionality -- just-so stories about
supervenience -- that are as-if. Darwinism is 100% functional, and self-sufficient;
it is the supervenient theology that is functionally superfluous there.
Alas, in the case of doings, it is the feelings that are functionally superfluous,
100% real though they are.
<br>&nbsp;
<blockquote><i><b>HUMPHREY</b>: A hundred and fifty years later no one
can be bothered with such sophistry. And there's a moral there.</i></blockquote>
I don't know whether any of this (about whether or not the hard nut has
indeed been cracked) is rightly called sophistry. Depends on which side
you see as bearing the burden of proof. Probably a bit premature to don
the mantle of Darwin, though...
<p>
</p><hr>
<p><b>REFERENCES</b>
</p><p>Geertz, C. (1973) The interpretation of cultures; selected essays. New
York, Basic Books
</p><p>Harnad, S. (1982) Consciousness: An afterthought. Cognition and Brain
Theory 5: 29 - 47. <a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad82.consciousness.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad82.consciousness.html</a>
</p><p>Harnad, S. (1990a) Against Computational Hermeneutics. (Invited commentary
on Eric Dietrich's Computationalism) Social Epistemology 4: 167-172. <a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad90.dietrich.crit.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad90.dietrich.crit.html</a>
</p><p>Harnad, S. (1990b) Lost in the hermeneutic hall of mirrors. Invited
Commentary on: Michael Dyer: Minds, Machines, Searle and Harnad. Journal
of Experimental and Theoretical Artificial Intelligence 2: 321 - 327. <a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad90.dyer.crit.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad90.dyer.crit.html</a>
</p><p>Harnad, S. (1994) Levels of Functional Equivalence in Reverse Bioengineering:
The Darwinian Turing Test for Artificial Life. Artificial Life 1(3): 293-301.
Reprinted in: C.G. Langton (Ed.). Artifial Life: An Overview. MIT Press
1995.
<br><a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad94.artlife2.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad94.artlife2.html</a>
</p><p>Harnad, S. (1995) Why and How We Are Not Zombies. Journal of Consciousness
Studies 1: 164-167. <a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad95.zombies.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad95.zombies.html</a>
</p><p>Harnad, S. (2000) Turing Indistinguishability and the Blind Watchmaker.
In: J. Fetzer &amp; Mulhauser, G. (eds.) "Evolving Consciousness" Amsterdam:
John Benjamins
<a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad98.turing.evol.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad98.turing.evol.html</a>
</p><p>Harnad, S. (2001) Minds, Machines, and Turing: The Indistinguishability
of Indistinguishables. Journal of Logic, Language, and Information (JoLLI)
special issue on "Alan Turing and Artificial Intelligence" (in press)
<br><a href="http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad00.turing.html">http://www.cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad00.turing.html</a>
</p><p>Humphrey, N. (2000) <a href="http://cogprints.soton.ac.uk/archives/phil/papers/200002/200002001/doc.html/mindbodytxt.htm">How
to Solve the Mind-Body Problem</a>"] <i>Journal of Consciousness Studies
</i>7.
<br><a href="http://cogprints.soton.ac.uk/abs/phil/200002001">http://cogprints.soton.ac.uk/abs/phil/200002001</a>
</p><p>Nagel, T. (1974) What is is like to be a bat? Philosophical Review 83:
435-451.
</p><p>Nagel, T. (1986) The view from nowhere. New York: Oxford University
Press


</p></body></html>